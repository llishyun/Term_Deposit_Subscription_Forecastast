{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQRoAGaEdori",
        "outputId": "6eee6545-dbe8-46af-b46a-aa4e26e37e5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WmeRRVSMvVu"
      },
      "source": [
        "#Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WmnyDgCZY3e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/ewha/pattern\"\n",
        "\n",
        "# Checking if our specified directory exists\n",
        "os.path.exists(root_dir)\n",
        "file_name = \"train.csv\"\n",
        "\n",
        "# Load the dataset using pandas\n",
        "df= pd.read_csv(os.path.join(root_dir, file_name))\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toHm66j8C31N"
      },
      "source": [
        "##variable selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R9VuLMXIPp5"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns='id')\n",
        "df = df.drop(columns='contact')\n",
        "df = df.drop(columns='pdays')\n",
        "df = df.drop(columns='emp.var.rate')\n",
        "df = df.drop(columns='day_of_week')\n",
        "df = df.drop(columns=\"default\")\n",
        "df = df.drop(columns='cons.conf.idx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2SYb3frmZd2"
      },
      "outputs": [],
      "source": [
        "df.replace('unknown', np.nan, inplace=True)\n",
        "df.replace('nonexistent', np.nan, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkJCZMrdGIGu"
      },
      "source": [
        "## Standardization and Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdZSvCZ7N3CB"
      },
      "source": [
        "* 규제의 효과 향상\n",
        "* 거리기반모델(knn,svm),경사 하강법 기반 모델(선형회귀, 로지스틱회귀, 인공신경망)에 특성 스케일링의 영향을 받음\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X98xLyoKGHdF"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Select numerical features\n",
        "numeric_features = df.select_dtypes(include=['int', 'float']).columns\n",
        "\n",
        "# Extract numerical data\n",
        "numeric_data = df[numeric_features]\n",
        "\n",
        "# Normalize numerical data\n",
        "normalizer = MinMaxScaler()\n",
        "normalized_data = normalizer.fit_transform(numeric_data)\n",
        "normalized_data = pd.DataFrame(normalized_data, columns=numeric_features)\n",
        "\n",
        "# Standardize normalized data , 스케일링\n",
        "scaler = StandardScaler()\n",
        "standardized_data = scaler.fit_transform(normalized_data)\n",
        "standardized_data = pd.DataFrame(standardized_data, columns=numeric_features)\n",
        "\n",
        "# Combine standardized numeric data with categorical data\n",
        "processed_data = pd.concat([standardized_data, df.drop(columns=numeric_features)], axis=1)\n",
        "df = processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAGY3A0XmqQI"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLbLCTwjoPBg"
      },
      "outputs": [],
      "source": [
        "df[\"y\"] = df[\"y\"].replace({\"no\": 0, \"yes\": 1})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = ['education', 'housing', 'loan', 'month', 'poutcome']"
      ],
      "metadata": {
        "id": "G-5gsuFbz8cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in label:\n",
        "    if df[col].dtype == 'object':\n",
        "        encoder = LabelEncoder()\n",
        "        df[col] = encoder.fit_transform(df[col])\n",
        "\n",
        "for col in label:\n",
        "    if df_test[col].dtype == 'object':\n",
        "        encoder = LabelEncoder()\n",
        "        df_test[col] = encoder.fit_transform(df_test[col])\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "z7K1Af2WztIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. OneHot Encoding"
      ],
      "metadata": {
        "id": "ReD2SrUYMVrG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crOpSvmfnWWm"
      },
      "outputs": [],
      "source": [
        "cat = ['job', 'marital']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "723kUmOvma2-"
      },
      "outputs": [],
      "source": [
        "encoded_df = pd.get_dummies(df, columns=cat)\n",
        "encoded_test = pd.get_dummies(df_test, columns=cat)\n",
        "\n",
        "encoded_df.replace({True: 1, False: 0}, inplace=True)\n",
        "encoded_test.replace({True: 1, False: 0}, inplace=True) # Encode True and False as 1 and 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXhjHQu3mtgT"
      },
      "outputs": [],
      "source": [
        "df = encoded_df\n",
        "df_test = encoded_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "W7UnmcFX1Gb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmX5sW2uXE1E"
      },
      "source": [
        "*missing value*\n",
        "- numerical은 예측 모델로 결측치 채우기\n",
        "* categorical은 최빈값으로 결측치 채우기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tnmJcAknm3Nv"
      },
      "outputs": [],
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, SimpleImputer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# MICE를 사용하여 결측치를 대체합니다.\n",
        "imputer = IterativeImputer()\n",
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "df = df_imputed\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "161KgkmiCMvj"
      },
      "source": [
        "outlier process\n",
        "* 고차원 데이터는 isolation이 적합\n",
        "outlier 인식 개수가 너무 차이나서 보수적으로 isolation 선택\n",
        "* dbscan through pca :13, isolation :659"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWoV6g1ufWbz"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "random_state = 100\n",
        "df_outlier = df.copy()\n",
        "features = df.columns.drop('y')\n",
        "\n",
        "iso_forest = IsolationForest(n_estimators=100, contamination=0.02, random_state=random_state)\n",
        "outliers = iso_forest.fit_predict(df_outlier[features]) #1,-1 value\n",
        "\n",
        "print(\"Outliers detected:\", np.sum(outliers == -1))\n",
        "df_outlier['outlier'] = outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZRaFCa8pe5U"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import pandas as pd\n",
        "\n",
        "def replace_outliers_with_model(df, features_df):\n",
        "    # 이상치를 포함한 데이터 복사\n",
        "    df_outliers = df.copy()\n",
        "\n",
        "    # Isolation Forest 모델 생성 및 학습\n",
        "    isolation_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=random_state)\n",
        "    outliers = isolation_forest.fit_predict(df_outliers[features_df])\n",
        "\n",
        "    # 예측을 위한 RandomForestRegressor 모델 생성\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "    for feature in features_df:\n",
        "        # 이상치 인덱스 추출\n",
        "        outlier_indices = df_outliers.index[outliers == -1]\n",
        "\n",
        "        if len(outlier_indices) > 0:\n",
        "            # 이상치를 제외한 데이터로 모델 학습\n",
        "            train_data = df_outliers.drop(outlier_indices)\n",
        "            rf_model.fit(train_data.drop(features_df, axis=1), train_data[feature])\n",
        "\n",
        "            # 이상치 예측\n",
        "            outlier_data = df_outliers.loc[outlier_indices].drop(features_df, axis=1)\n",
        "            predicted_outliers = rf_model.predict(outlier_data)\n",
        "\n",
        "            # 예측된 값으로 이상치 대체\n",
        "            df_outliers.loc[outlier_indices, feature] = predicted_outliers\n",
        "\n",
        "    return df_outliers\n",
        "\n",
        "features_df = df.columns.drop('y')\n",
        "\n",
        "# 이상치를 예측하고 예측된 값으로 대체\n",
        "df = replace_outliers_with_model(df, features_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFCoi7B5fT4-"
      },
      "outputs": [],
      "source": [
        "df_outlier = df.copy()\n",
        "\n",
        "iso_forest = IsolationForest(n_estimators=100, contamination=0.02, random_state=random_state) #contamination이 뭔지 확인\n",
        "outliers = iso_forest.fit_predict(df_outlier[features]) #1,-1 value\n",
        "\n",
        "print(\"Outliers detected:\", np.sum(outliers == -1))\n",
        "df_outlier['outlier'] = outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29YZ6H10n0hn"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "X = df.drop('y',axis=1)\n",
        "\n",
        "# PCA로 차원 축소\n",
        "pca = PCA(n_components=2)  # 필요한 차원 수로 설정\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "# K-means 클러스터링 적용\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "clusters = kmeans.fit_predict(X_reduced)\n",
        "df['cluster'] = clusters\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAqpzArd3m1D"
      },
      "source": [
        "data split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on0SfMJd3l2c"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "shuffle = True\n",
        "test_size_ratio = 0.18 # The optimal test dataset ratio is specified as 0.25.(by experience)\n",
        "\n",
        "X = df.drop('y', axis =1)\n",
        "y=pd.Series(df.y)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_ratio, random_state=random_state, shuffle=shuffle)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPT-xc8sNqWG"
      },
      "source": [
        "Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uNGto58NszO"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from lightgbm import LGBMRegressor, LGBMClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
        "scoring = \"neg_mean_squared_error\"\n",
        "\n",
        "models = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# 데이터 준비 (예제로 랜덤 데이터 생성)\n",
        "# X_train, y_train, X_test, y_test 등의 데이터가 있다고 가정합니다.\n",
        "\n",
        "# 모델 정의\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(26,)),  # 입력 레이어\n",
        "    layers.Dense(32, activation='relu'),  # 은닉 레이어\n",
        "    layers.Dense(2, activation='softmax')  # 출력 레이어 (다중 클래스 분류에서는 softmax 사용)\n",
        "])\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam',  # 옵티마이저\n",
        "              loss='sparse_categorical_crossentropy',  # 손실 함수 (다중 클래스 분류에서는 sparse_categorical_crossentropy 사용)\n",
        "              metrics=['accuracy'])  # 평가 지표\n",
        "\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# 모델 평가\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "# F1 점수 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f1)\n"
      ],
      "metadata": {
        "id": "3wN5tWFv4QJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk6l2AHxNjkX"
      },
      "outputs": [],
      "source": [
        "#decisionTree\n",
        "model = DecisionTreeRegressor(random_state=random_state)\n",
        "\n",
        "# Define the hyperparameters and their possible values\n",
        "param_grid = {\n",
        "    \"max_depth\": [5, 10, 20],\n",
        "    \"min_samples_split\": [2, 10, 20],\n",
        "    \"ccp_alpha\": [0.0, 0.01],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best CV score: {:.6f}\".format(grid_search.best_score_))\n",
        "\n",
        "models[\"Decision Tree\"] = grid_search.best_estimator_\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# F1 점수 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6noqtzPUWJM"
      },
      "outputs": [],
      "source": [
        "# DecisionTree Classifier\n",
        "model = DecisionTreeClassifier(random_state=13)\n",
        "\n",
        "param_grid = {\n",
        "    \"criterion\": [\"gini\", \"entropy\"],\n",
        "    \"max_depth\": [None, 10, 20, 30],\n",
        "    \"min_samples_split\": [2, 10, 20],\n",
        "    \"min_samples_leaf\": [1, 5, 10],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=kf, scoring='accuracy', refit=True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best CV score: {:.6f}\".format(grid_search.best_score_))\n",
        "\n",
        "models[\"Decision Tree\"] = grid_search.best_estimator_\n",
        "\n",
        "y_prob = grid_search.predict_proba(X_test)\n",
        "y_pred =grid_search.predict(X_test)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# F1 점수 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f1)\n",
        "\n",
        "\"\"\"print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1:\", f1_score(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob[:, 1]))\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49nX-oAlOdnv"
      },
      "outputs": [],
      "source": [
        "# baggingRegressor\n",
        "base_model = DecisionTreeRegressor()\n",
        "model = BaggingRegressor(estimator=base_model,\n",
        "                         bootstrap=True,\n",
        "                         n_jobs=-1,\n",
        "                         random_state=random_state)\n",
        "\n",
        "# Define the hyperparameters and their possible values\n",
        "param_grid = {\n",
        "    \"n_estimators\": [25, 50]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best CV score: {:.6f}\".format(grid_search.best_score_))\n",
        "\n",
        "models[\"Bagging\"] = grid_search.best_estimator_\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# F1 점수 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC_NYtmsU2ZB"
      },
      "outputs": [],
      "source": [
        "# Bagging Classifier\n",
        "base_model = DecisionTreeClassifier()\n",
        "model = BaggingClassifier(estimator=base_model,\n",
        "                         bootstrap=True,\n",
        "                         n_jobs=-1,\n",
        "                         random_state=random_state)\n",
        "\n",
        "# Define the hyperparameters and their possible values\n",
        "param_grid = {\n",
        "    \"n_estimators\": [25, 50]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best CV score: {:.6f}\".format(grid_search.best_score_))\n",
        "\n",
        "models[\"Bagging\"] = grid_search.best_estimator_\n",
        "\n",
        "y_prob = grid_search.predict_proba(X_test)\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "\"\"\"print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1:\", f1_score(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob[:, 1]))\"\"\"\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# F1 점수 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOBpW5LdOycT"
      },
      "outputs": [],
      "source": [
        "#randomforest\n",
        "model = RandomForestRegressor(max_depth=None,\n",
        "                              min_samples_split=2,\n",
        "                              bootstrap=True,\n",
        "                              n_jobs=-1,\n",
        "                              random_state=random_state)\n",
        "\n",
        "# Define the hyperparameters and their possible values\n",
        "param_grid = {\n",
        "    \"n_estimators\": [25, 50],\n",
        "    \"max_features\": [0.5, \"sqrt\", \"log2\", None],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best CV score: {:.6f}\".format(grid_search.best_score_))\n",
        "\n",
        "models[\"Random Forest\"] = grid_search.best_estimator_\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# F1 점수 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49Sh-DaUVVX7"
      },
      "outputs": [],
      "source": [
        "# AdaBoostClassifier 모델 생성\n",
        "model = AdaBoostClassifier(random_state=random_state)\n",
        "\n",
        "# 하이퍼파라미터 그리드 정의\n",
        "param_grid = {\n",
        "    \"n_estimators\": [25, 50],\n",
        "    \"base_estimator\": [DecisionTreeClassifier(max_depth=3), DecisionTreeClassifier(max_depth=6)],\n",
        "    \"learning_rate\": [0.1, 1.0],\n",
        "}\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best CV score: {:.6f}\".format(grid_search.best_score_))\n",
        "\n",
        "models[\"AdaBoost\"] = grid_search.best_estimator_\n",
        "\n",
        "y_prob = grid_search.predict_proba(X_test)\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "\"\"\"print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1:\", f1_score(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob[:, 1]))\"\"\"\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# F1 점수 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pHXdisnO9rH"
      },
      "outputs": [],
      "source": [
        "#GB\n",
        "model = GradientBoostingRegressor(loss=\"squared_error\",\n",
        "                                  subsample=1.0,\n",
        "                                  random_state=random_state)\n",
        "\n",
        "# Define the hyperparameters and their possible values\n",
        "param_grid = {\n",
        "    \"n_estimators\": [25, 50],\n",
        "    \"max_depth\": [3, 6],\n",
        "    \"learning_rate\": [0.0, 0.1],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best CV score: {:.6f}\".format(grid_search.best_score_))\n",
        "\n",
        "models[\"Gradient Boosting\"] = grid_search.best_estimator_\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# F1 점수 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcqNiwqTPHUU"
      },
      "outputs": [],
      "source": [
        "#XGB Regressor\n",
        "model = XGBRegressor(subsample=1.0,\n",
        "                     learning_rate=0.1,\n",
        "                     max_depth=7,\n",
        "                     n_jobs=-1,\n",
        "                     random_state=random_state)\n",
        "\n",
        "# Define the hyperparameters and their possible values\n",
        "param_grid = {\n",
        "    \"n_estimators\": [25, 50],\n",
        "    \"reg_alpha\": [0, 0.1],\n",
        "    \"reg_lambda\": [0, 0.1],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best CV score: {:.6f}\".format(grid_search.best_score_))\n",
        "\n",
        "models[\"XGBoost\"] = grid_search.best_estimator_\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# F1 점수 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WbvYpxYVBvT"
      },
      "outputs": [],
      "source": [
        "# XGB classifier\n",
        "model = XGBClassifier(subsample=1.0,\n",
        "                     learning_rate=0.1,\n",
        "                     max_depth=6,\n",
        "                     n_jobs=-1,\n",
        "                     random_state=random_state)\n",
        "\n",
        "# Define the hyperparameters and their possible values\n",
        "param_grid = {\n",
        "    \"n_estimators\": [25, 50],\n",
        "    \"reg_alpha\": [0, 0.1],  # L1 정규화\n",
        "    \"reg_lambda\": [0, 0.1],  # L2 정규화\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best CV score: {:.6f}\".format(grid_search.best_score_))\n",
        "\n",
        "models[\"XGBoost\"] = grid_search.best_estimator_\n",
        "\n",
        "y_prob = grid_search.predict_proba(X_test)\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "\"\"\"print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1:\", f1_score(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob[:, 1]))\"\"\"\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# F1 점수 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9FgPJXmHv7p"
      },
      "outputs": [],
      "source": [
        "#LGBMRegressor\n",
        "\n",
        "model = LGBMRegressor(learning_rate=0.1,\n",
        "                      data_sample_strategy=\"goss\",\n",
        "                      top_rate=0.2,\n",
        "                      other_rate=0.1,\n",
        "                      force_col_wise=True,\n",
        "                      verbosity=0,\n",
        "                      n_jobs=-1,\n",
        "                      random_state=random_state)\n",
        "\n",
        "\n",
        "# Define the hyperparameters and their possible values\n",
        "param_grid = {\n",
        "    \"n_estimators\": [25, 50],\n",
        "    \"reg_alpha\": [0, 0.1],\n",
        "    \"reg_lambda\": [0, 0.1],\n",
        "    \"enable_bundle\": [True, False]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best CV score: {:.6f}\".format(grid_search.best_score_))\n",
        "\n",
        "models[\"LightGBM\"] = grid_search.best_estimator_\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# F1 점수 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtVSYCf8VLJJ"
      },
      "outputs": [],
      "source": [
        "# LGBM Classifier\n",
        "model = LGBMClassifier(learning_rate=0.1,\n",
        "                      data_sample_strategy=\"goss\",\n",
        "                      top_rate=0.2,\n",
        "                      other_rate=0.1,\n",
        "                      force_col_wise=True,\n",
        "                      verbosity=0,\n",
        "                      n_jobs=-1,\n",
        "                      random_state=random_state)\n",
        "\n",
        "# Define the hyperparameters and their possible values\n",
        "param_grid = {\n",
        "    \"n_estimators\": [25, 50],\n",
        "    \"reg_alpha\": [0, 0.1],\n",
        "    \"reg_lambda\": [0, 0.1],\n",
        "    \"enable_bundle\": [True, False]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best CV score: {:.6f}\".format(grid_search.best_score_))\n",
        "\n",
        "models[\"LightGBM\"] = grid_search.best_estimator_\n",
        "\n",
        "y_prob = grid_search.predict_proba(X_test)\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "\"\"\"print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1:\", f1_score(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob[:, 1]))\"\"\"\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# F1 점수 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imU4ePzGNf6S"
      },
      "outputs": [],
      "source": [
        "#Neuron Network\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 랜덤 시드 고정\n",
        "seed_value = 100\n",
        "np.random.seed(seed_value)\n",
        "tf.random.set_seed(seed_value)\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 신경망 모델 생성 함수\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 교차 검증 수행 및 가장 높은 F1 스코어를 가진 모델 선택\n",
        "f1_scores = []\n",
        "models1 = []\n",
        "\n",
        "for train_index, val_index in kf.split(X_train):\n",
        "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
        "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "\n",
        "    model = create_model()\n",
        "    model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=128, verbose=0) #파라미터 값에 따라 정확도 달라짐\n",
        "\n",
        "    y_pred_prob = model.predict(X_val_fold)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "    f1 = f1_score(y_val_fold, y_pred)\n",
        "\n",
        "    f1_scores.append(f1)\n",
        "    models1.append(model)\n",
        "\n",
        "best_model_index = np.argmax(f1_scores)\n",
        "mean_f1_score = np.mean(f1_scores)\n",
        "best_model = models1[best_model_index]\n",
        "\n",
        "#K-fold로 여러 모델의 검증 점수를 평균 내어 best 모델을 select\n",
        "models[\"Neuron Network\"] = best_model\n",
        "\n",
        "print(f'Best F1 Score: {f1_scores[best_model_index]}')\n",
        "print(f'Mean F1 Score: {mean_f1_score}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubH-ua6psIbd"
      },
      "outputs": [],
      "source": [
        "#서포트 벡터 머신(Support Vector Machine, SVM) - L2 규제 내장되어있음\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "model = SVC(kernel='rbf', C=1.0)  # RBF 커널을 사용하는 SVM 모델, C는 규제 매개변수\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "models[\"SVM\"]=model\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# F1 점수 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "t1FkODbh-zx6"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 모델 정의\n",
        "model = MLPClassifier(hidden_layer_sizes=(40, 30),\n",
        "                      max_iter=400,\n",
        "                      activation='relu',\n",
        "                      solver='adam',\n",
        "                      batch_size=200,\n",
        "                      learning_rate='invscaling',\n",
        "                      learning_rate_init=0.01,\n",
        "                      power_t=0.5,\n",
        "                      warm_start=True,\n",
        "                      random_state=100,\n",
        "                      alpha = 0.01,\n",
        "                      verbose=True)\n",
        "\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Access the loss_curve_ attribute\n",
        "loss_values = model.loss_curve_\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# F1 점수 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYYTsmCKvFxx"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Polynomial Regression 모델 생성\n",
        "model = make_pipeline(PolynomialFeatures(), LinearRegression(n_jobs=-1))\n",
        "\n",
        "#model.named_steps['polynomialfeatures'].degree = 3  # 다항식의 차수를 3으로 설정\n",
        "\n",
        "order = 3\n",
        "# PolynomialFeatures 단계의 order 매개변수를 설정합니다.\n",
        "model.named_steps['polynomialfeatures'].order = 'C'\n",
        "\n",
        "\"\"\"# Define the hyperparameters and their possible values\n",
        "param_grid = {\n",
        "    'polynomialfeatures__degree': [2, 3]  # 다항식 차수\n",
        "}\n",
        "\n",
        "# GridSearchCV를 사용하여 최적의 모델 및 하이퍼파라미터 찾기\n",
        "grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 최적의 모델 및 하이퍼파라미터 출력\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best CV score: {:.6f}\".format(grid_search.best_score_))\n",
        "\n",
        "# 테스트 데이터에 대해 최적의 모델 사용하여 예측\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "0.5246142542248348\n",
        "\"\"\"\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "f1_r = f1_score(y_test, predictions)\n",
        "print(f1_r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOrVuZmOPof2"
      },
      "source": [
        "mlp 0.517\n",
        "* 0.5249\n",
        "* 0.5323 : learning rate : 0.01, alpha = 0.01 적합"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Luq31mgCDc2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# 예측값이 담긴 리스트나 배열을 판다스 데이터프레임으로 변환\n",
        "predictions_df = pd.DataFrame({'y_pred': y_pred})\n",
        "\n",
        "# 2. 테스트 데이터의 'id' 열을 가져와 예측값과 결합하여 하나의 데이터프레임으로 만듦\n",
        "result_data = pd.read_csv('test.csv')  # 테스트 데이터 불러오기\n",
        "result = pd.concat([result_data['id'], predictions_df], axis=1)\n",
        "# 'id'와 예측값을 결합하여 하나의 데이터프레임으로 만듦\n",
        "# axis=1 : 열 방향 결합\n",
        "\n",
        "# 3. 결과를 CSV 파일로 저장\n",
        "result.to_csv('result.csv', index=False)  # index=False로 설정하여 인덱스를 CSV 파일에 저장하지 않음\n",
        "\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
